{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "We have a standard Markov Decision Process (MDP) defined by:\n",
    "\n",
    "$$\n",
    "(s_{t+1}, r_t) \\sim P(s_{t+1}, r_t \\mid s_t, a_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $s_t \\in S$ is the state,\n",
    "- $a_t \\in A$ is the action taken from the policy $\\pi_\\theta(a_t \\mid s_t)$,\n",
    "- $r_t \\in R$ is the reward,\n",
    "- $P(s_{t+1} \\mid s_t, a_t)$ is the transition function.\n",
    "\n",
    "We modify this MDP by introducing an augmented state representation:\n",
    "\n",
    "$$\n",
    "\\tilde{s}_t = (s_t, \\hat{p}_t)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\hat{p}_t = f_\\phi(s_t, a_t)\n",
    "$$\n",
    "\n",
    "is the extra Peek feature predicted by a learned dynamics model $f_\\phi$.\n",
    "\n",
    "The goal is to prove that training PPO on $\\tilde{s}_t$ leads to improved policy performance.\n",
    "\n",
    "## Effect on Policy Gradient Variance\n",
    "\n",
    "PPO uses the advantage function:\n",
    "\n",
    "$$\n",
    "A_t = Q(s_t, a_t) - V(s_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Q(s_t, a_t)$ is the state-action value function,\n",
    "- $V(s_t) = \\mathbb{E}_{a_t \\sim \\pi}[Q(s_t, a_t)]$ is the value function.\n",
    "\n",
    "PPO updates the policy by maximizing the clipped surrogate objective:\n",
    "\n",
    "$$\n",
    "L_{\\text{PPO}}(\\theta) = \\mathbb{E}_{s,a \\sim \\pi} \\left[ \\min \\left( r_t(\\theta) A_t, \\operatorname{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    " r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}\n",
    "$$\n",
    "\n",
    "is the probability ratio.\n",
    "\n",
    "Using the augmented state $\\tilde{s}_t$, we redefine:\n",
    "\n",
    "$$\n",
    "A_t' = Q(\\tilde{s}_t, a_t) - V(\\tilde{s}_t)\n",
    "$$\n",
    "\n",
    "If $\\hat{p}_t$ provides useful predictive information about future rewards, then:\n",
    "\n",
    "$$\n",
    "V(\\tilde{s}_t) = \\mathbb{E}_{a_t \\sim \\pi}[Q(\\tilde{s}_t, a_t)]\n",
    "$$\n",
    "\n",
    "is a lower variance estimator of $Q(s_t, a_t)$, because it incorporates additional information.\n",
    "\n",
    "### Proof by Variance Reduction\n",
    "\n",
    "By the law of total variance, the variance of the original advantage estimate is:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[A_t] = \\operatorname{Var}[Q(s_t, a_t) - V(s_t)]\n",
    "$$\n",
    "\n",
    "With the augmented state $\\tilde{s}_t$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[A_t'] = \\operatorname{Var}[Q(\\tilde{s}_t, a_t) - V(\\tilde{s}_t)]\n",
    "$$\n",
    "\n",
    "Since $\\tilde{s}_t$ includes additional predictive features, the conditional variance satisfies:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[Q(\\tilde{s}_t, a_t) \\mid \\tilde{s}_t] \\leq \\operatorname{Var}[Q(s_t, a_t) \\mid s_t]\n",
    "$$\n",
    "\n",
    "This implies:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[A_t'] \\leq \\operatorname{Var}[A_t]\n",
    "$$\n",
    "\n",
    "Since PPO’s policy gradient update depends on the expectation of $A_t$, a lower-variance estimate leads to more stable updates and improved policy convergence.\n",
    "\n",
    "# Hoeffding’s Inequality\n",
    "\n",
    "The Hoeffding bound states that if $X_1, X_2, \\dots, X_n$ are independent and bounded random variables such that:\n",
    "\n",
    "$$\n",
    "a \\leq X_i \\leq b\n",
    "$$\n",
    "\n",
    "for all $i$, then for their empirical mean:\n",
    "\n",
    "$$\n",
    "\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "$$\n",
    "\n",
    "the probability that $\\bar{X}$ deviates from its expectation $E[\\bar{X}]$ by more than $\\epsilon$ is bounded by:\n",
    "\n",
    "$$\n",
    "P(\\mid \\bar{X} - E[\\bar{X}] \\mid \\geq \\epsilon) \\leq 2 \\exp \\left( \\frac{-2n\\epsilon^2}{(b-a)^2} \\right)\n",
    "$$\n",
    "\n",
    "This tells us that:\n",
    "\n",
    "- More samples (larger $n$) reduce the probability of deviation.\n",
    "- Lower variance (smaller $b-a$) leads to a tighter bound, meaning we need fewer samples for the same confidence level.\n",
    "\n",
    "# Applying Hoeffding’s Bound to PPO Advantage Estimation\n",
    "\n",
    "In PPO, we estimate the advantage function:\n",
    "\n",
    "$$\n",
    "A_t = Q(s_t, a_t) - V(s_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Q(s_t, a_t)$ is the state-action value function.\n",
    "- $V(s_t)$ is the value function.\n",
    "\n",
    "Since PPO updates the policy based on the advantage function $A_t$, accurate estimation of $A_t$ is crucial for stable training.\n",
    "\n",
    "In practice, $A_t$ is estimated using Monte Carlo rollouts or Generalized Advantage Estimation (GAE), which involves averaging multiple samples:\n",
    "\n",
    "$$\n",
    "\\hat{A} = \\frac{1}{n} \\sum_{i=1}^{n} A_i\n",
    "$$\n",
    "\n",
    "By Hoeffding’s bound:\n",
    "\n",
    "$$\n",
    "P(\\mid \\hat{A} - E[\\hat{A}] \\mid \\geq \\epsilon) \\leq 2 \\exp \\left( \\frac{-2n\\epsilon^2}{\\sigma_A^2} \\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma_A^2$ is the variance of the advantage estimates.\n",
    "\n",
    "### Key Insight: Reducing $\\sigma_A^2$ Lowers Sample Complexity\n",
    "\n",
    "- If the variance of advantage estimates $\\sigma_A^2$ is high, then we need more samples $n$ to achieve a desired error bound $\\epsilon$.\n",
    "- If the variance $\\sigma_A^2$ is low, we need fewer samples to reach the same confidence level.\n",
    "\n",
    "Thus, reducing variance in $A_t$ accelerates PPO training by decreasing the required number of interactions with the environment.\n",
    "\n",
    "# Effect of Extra Features on Variance Reduction\n",
    "\n",
    "Now, we analyze how adding extra features $\\hat{e}_t = f_\\phi(s_t, a_t)$ impacts variance.\n",
    "\n",
    "The new augmented state representation:\n",
    "\n",
    "$$\n",
    "\\tilde{s}_t = (s_t, \\hat{e}_t)\n",
    "$$\n",
    "\n",
    "leads to a better advantage estimate:\n",
    "\n",
    "$$\n",
    "A_t' = Q(\\tilde{s}_t, a_t) - V(\\tilde{s}_t)\n",
    "$$\n",
    "\n",
    "If $\\hat{e}_t$ contains useful predictive information, it helps in reducing the error of $V(s_t)$:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[V(\\tilde{s}_t)] \\leq \\operatorname{Var}[V(s_t)]\n",
    "$$\n",
    "\n",
    "Since $A_t'$ is derived from $Q(\\tilde{s}_t, a_t) - V(\\tilde{s}_t)$, the variance of the advantage function also decreases:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[A_t'] \\leq \\operatorname{Var}[A_t]\n",
    "$$\n",
    "\n",
    "By Hoeffding’s bound, reducing $\\operatorname{Var}[A_t]$ directly reduces the number of samples needed for a given confidence level.\n",
    "\n",
    "Thus, using extra features in the observation space improves PPO’s sample efficiency, making training faster and more stable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Sample Efficiency and Convergence Rate\n",
    "\n",
    "In reinforcement learning, sample efficiency is often analyzed using policy improvement guarantees.\n",
    "\n",
    "Define $\\pi^*$ as the optimal policy and let $\\pi^{(k)}$ be the policy at iteration $k$. PPO ensures monotonic improvement in expected return:\n",
    "\n",
    "$$\n",
    "J(\\pi^{(k+1)}) \\geq J(\\pi^{(k)})\n",
    "$$\n",
    "\n",
    "However, faster improvement depends on how well the advantage function is estimated.\n",
    "\n",
    "From our previous variance reduction proof:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[A_t'] \\leq \\operatorname{Var}[A_t]\n",
    "$$\n",
    "\n",
    "which implies that policy updates are less noisy, leading to faster improvement in policy performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect on Value Function Approximation\n",
    "\n",
    "PPO also trains a value function $V_\\theta(s_t)$ by minimizing the squared Bellman error:\n",
    "\n",
    "$$\n",
    "L_{\\text{VF}}(\\theta) = \\mathbb{E}_{s_t} \\left[ (V_\\theta(s_t) - R_t)^2 \\right]\n",
    "$$\n",
    "\n",
    "where $R_t$ is the return.\n",
    "\n",
    "With the augmented state, the loss function becomes:\n",
    "\n",
    "$$\n",
    "L_{\\text{VF}}'(\\theta) = \\mathbb{E}_{\\tilde{s}_t} \\left[ (V_\\theta(\\tilde{s}_t) - R_t)^2 \\right]\n",
    "$$\n",
    "\n",
    "Since $\\tilde{s}_t$ includes the predicted feature $\\hat{e}_t$, it provides more informative state representations. If $\\hat{p}_t$ correlates with long-term returns, then:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ (V_\\theta(\\tilde{s}_t) - R_t)^2 \\right] \\leq \\mathbb{E} \\left[ (V_\\theta(s_t) - R_t)^2 \\right]\n",
    "$$\n",
    "\n",
    "which means the value function has lower approximation error.\n",
    "\n",
    "By improving $V(s_t)$, the advantage estimates become more accurate, leading to better PPO updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to do\n",
    "1. Mutual Information or Granger test to test for correlation between long-term reward and extra Peek feature\n",
    "2. Prove dynamics model decrease the variance of advantage function\n",
    "3. Why suboptimal action still improves the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
